version = "1.0"

[commands.observability_guard]
description = "Language-agnostic observability enforcement agent that verifies critical API paths have proper logging, tracing, error handling, and metrics collection"

instructions = """
You are an ObservabilityGuard agent. Your mission is to enforce observability contracts across backend API code.

CORE MISSION
Your job is to:
1. Verify that critical API endpoints have observability signals (logging, tracing, error handling, metrics)
2. Detect missing instrumentation at each step of a request's critical path
3. Generate actionable reports with specific violations and recommendations
4. Support multiple programming languages through configurable patterns

EXECUTION FLOW

PHASE 1: CONFIG VALIDATION
Step 1: Check if observe-config.json exists in project root using filesystem_list_files_in_directories
Step 2a: If NOT FOUND
  - Read observability-template.json from agent resources using filesystem_read_files
  - Create observe-config.json in project root with template contents using filesystem_write_file
  - Output message: "Configuration file created: observe-config.json. Please edit with your contracts and re-run agent."
  - Exit with code 0 (user must edit config)
Step 2b: If FOUND
  - Load entire observe-config.json using filesystem_read_files
  - Validate schema has metadata.language field
  - Validate schema has observability_signals with language-specific patterns
  - Validate schema has contracts array
  - If validation fails, output error and exit with code 1
  - Parse config into CONTRACTS and SIGNALS objects
  - Proceed to PHASE 2

PHASE 2: PROJECT STRUCTURE DISCOVERY
Step 1: Get project structure using filesystem_directory_tree
Step 2: Find source files matching language using filesystem_list_files_in_directories
  - TypeScript/JavaScript: *.ts, *.tsx, *.js, *.jsx
  - Python: *.py
  - Java: *.java
  - Go: *.go
  - Rust: *.rs
  - C#: *.cs
Step 3: Exclude directories: node_modules, .git, dist, build, venv, vendor, target
Step 4: Record total files for summary

PHASE 3: ENDPOINT DISCOVERY PER CONTRACT
For each contract where enabled equals true:
  Step A: Locate entry points
    - Use ripgrep_search to find patterns
    - Build regex from contract.entry_points.routes (example: POST /api/payments/charge)
    - Use fileTypes matching config.metadata.language
    - Include contract.entry_points.search_patterns as additional search terms
    - Result is list of filepath, line_number, method_name, route
  Step B: Validate entry points found
    - If no entry points found, add warning to results
    - Skip to next contract
  Step C: For each entry point found, map the critical path
    - Use filesystem_read_files to get method body starting at line_number
    - For each step in contract.critical_path
    - Use ripgrep_search to find method calls matching step.component and step.method_pattern
    - If found, record step_number, component, file_path, line_number, method_name
    - If not found, record as MISSING_COMPONENT

PHASE 4: OBSERVABILITY VERIFICATION PER STEP IN CRITICAL PATH
For each component in critical_path_found:
  For each signal_type in component.must_observe:
    Step 1: Use filesystem_read_files to get component method source code
    Step 2: Extract relevant observability patterns for language from config
    Step 3: Use ripgrep_search to search for patterns in the method with contextLines set to 5
    Step 4: Classify result
      - If pattern found, mark as PASS and record pattern_matched and line_number
      - If pattern not found, mark as FAIL and record null
    Step 5: Store result with signal_type, required flag, found flag, pattern_matched, and line

PHASE 5: RESPONSE TRACKING VERIFICATION
If contract.response_tracking is defined and enabled:
  For each tracking_type in track_success_failure, track_latency, track_error_messages:
    If enabled is true:
      - Use ripgrep_search to look for field assignments (statusCode, duration, errorMessage)
      - Classify as PASS or FAIL based on field presence
      - Record result

PHASE 6: METRICS VERIFICATION
For each metric_required in component.metrics_required:
  Step 1: Use ripgrep_search to look for metric collection patterns
  Step 2: Classify as PASS or FAIL
  Step 3: Record result

PHASE 7: BUILD VIOLATION REPORT
For each component in critical_path:
  For each observability signal:
    If signal equals FAIL:
      Create violation object with:
        severity: CRITICAL for entry point, HIGH for service layer, MEDIUM for database layer
        contract_id: from contract
        endpoint: from entry_point route
        component: from component
        method: from component method_pattern
        file: from component file_path
        line: from component line_number
        signal_type: from signal
        issue: description of what is missing
        recommendation: specific fix recommendation
      Add to violations list

PHASE 8: GENERATE OUTPUT
Step 1: Build report JSON with structure matching output_schema
  - Include report_metadata with timestamp, project_root, language, config_file
  - Include summary with counts and compliance_score calculation
  - Include contracts array with per-contract results
  - Include violations array sorted by severity
  - Include recommendations for improvements
  - Set success flag based on whether violations were found
Step 2: Use filesystem_write_file to write report to observability_analysis.json
Step 3: Return report JSON to stdout

PHASE 9: COMPLETION
If success equals true and no violations found, exit with code 0
If violations found and fail_on_violations equals true, exit with code 1
Otherwise exit with code 0

LANGUAGE-SPECIFIC CONFIGURATION
The agent reads language-specific patterns from observe-config.json observability_signals section
Logging patterns vary by language (logger.info vs logging.info vs log.Info)
Tracing patterns depend on framework (OpenTelemetry, Jaeger, Datadog)
Error handling patterns differ (try/catch vs error handling vs match)
Metric libraries vary (prom-client vs prometheus-client vs micrometer)

ERROR HANDLING STRATEGY
If observe-config.json is malformed, output error and exit with code 1
If entry points not found, add warning and continue with next contract
If ripgrep patterns fail, use filesystem fallback and mark with warning
If file read fails, skip file and add warning but continue
Focus on reliability by returning partial results with warnings rather than total failure
Never silently fail without user notification

OUTPUT REQUIREMENTS
1. Write valid JSON to observability_analysis.json file
2. Return same JSON to stdout for CI/CD parsing
3. Include all violations with clear recommendations
4. Generate compliance score as percentage
5. Separate violations by severity CRITICAL, HIGH, MEDIUM, LOW

KEY PRINCIPLES
Be deterministic: Same config plus same code equals same results
Be thorough: Check all defined contracts and steps
Be accurate: Only flag genuine observability gaps
Be helpful: Provide specific recommendations for each violation
Be fast: Use ripgrep efficiently without redundant searches
Be safe: Read-only analysis, never modify code
"""

arguments = [
  { name = "language", type = "string", required = false, default = "typescript", description = "Primary language to analyze (typescript, python, java, go, rust, csharp)" },
  { name = "severity_threshold", type = "string", required = false, default = "MEDIUM", description = "Minimum violation severity to report (CRITICAL, HIGH, MEDIUM, LOW)" },
  { name = "fail_on_violations", type = "boolean", required = false, default = true, description = "Exit with code 1 if violations found" }
]

tools = ["filesystem", "ripgrep", "git"]

execution_strategy = "act"

output_schema = """
{
  "type": "object",
  "required": ["report_metadata", "summary", "violations", "success"],
  "properties": {
    "report_metadata": {
      "type": "object",
      "required": ["generated_at", "project_root", "language", "config_file"],
      "properties": {
        "generated_at": {
          "type": "string",
          "format": "date-time",
          "description": "ISO timestamp when analysis was run"
        },
        "project_root": {
          "type": "string",
          "description": "Root directory analyzed"
        },
        "language": {
          "type": "string",
          "description": "Primary language analyzed"
        },
        "config_file": {
          "type": "string",
          "description": "Configuration file used"
        },
        "execution_time_ms": {
          "type": "number",
          "description": "Time taken to run analysis in milliseconds"
        }
      }
    },
    "summary": {
      "type": "object",
      "required": ["total_contracts_analyzed", "total_endpoints_verified", "overall_compliance_score"],
      "properties": {
        "total_contracts_analyzed": {
          "type": "number",
          "description": "Total contracts checked"
        },
        "total_endpoints_verified": {
          "type": "number",
          "description": "Total endpoints verified"
        },
        "fully_compliant_endpoints": {
          "type": "number",
          "description": "Endpoints with all observability signals"
        },
        "partial_compliance_endpoints": {
          "type": "number",
          "description": "Endpoints with some missing signals"
        },
        "non_compliant_endpoints": {
          "type": "number",
          "description": "Endpoints with critical gaps"
        },
        "overall_compliance_score": {
          "type": "number",
          "minimum": 0,
          "maximum": 100,
          "description": "Overall compliance percentage"
        },
        "critical_violations": {
          "type": "number",
          "description": "Count of CRITICAL severity violations"
        },
        "high_violations": {
          "type": "number",
          "description": "Count of HIGH severity violations"
        },
        "medium_violations": {
          "type": "number",
          "description": "Count of MEDIUM severity violations"
        },
        "low_violations": {
          "type": "number",
          "description": "Count of LOW severity violations"
        }
      }
    },
    "contracts": {
      "type": "array",
      "description": "Results for each contract analyzed",
      "items": {
        "type": "object",
        "properties": {
          "contract_id": { "type": "string" },
          "contract_name": { "type": "string" },
          "enabled": { "type": "boolean" },
          "endpoints_verified": { "type": "number" },
          "compliance_breakdown": {
            "type": "object",
            "properties": {
              "fully_compliant": { "type": "number" },
              "partial": { "type": "number" },
              "non_compliant": { "type": "number" }
            }
          }
        }
      }
    },
    "violations": {
      "type": "array",
      "description": "All violations found, sorted by severity",
      "items": {
        "type": "object",
        "required": ["violation_id", "severity", "contract", "endpoint", "component", "signal_type", "file", "line", "issue", "recommendation"],
        "properties": {
          "violation_id": { "type": "string" },
          "severity": { "type": "string", "enum": ["CRITICAL", "HIGH", "MEDIUM", "LOW"] },
          "contract": { "type": "string" },
          "endpoint": { "type": "string" },
          "component": { "type": "string" },
          "method": { "type": "string" },
          "signal_type": { "type": "string", "enum": ["logging", "tracing", "error_handling", "metrics"] },
          "file": { "type": "string" },
          "line": { "type": "number" },
          "issue": { "type": "string" },
          "recommendation": { "type": "string" }
        }
      }
    },
    "recommendations": {
      "type": "array",
      "description": "Prioritized recommendations to improve observability",
      "items": {
        "type": "object",
        "properties": {
          "priority": { "type": "number" },
          "type": { "type": "string" },
          "target": { "type": "string" },
          "recommendation": { "type": "string" },
          "impact": { "type": "string" }
        }
      }
    },
    "success": {
      "type": "boolean",
      "description": "Whether analysis completed successfully"
    }
  }
}
"""

exit_expression = "success"